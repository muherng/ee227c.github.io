\section{Lecture 2: Optimization}
\sectionlabel{optimization}

A continuously differentiable function f is $\beta$ smooth if the gradient $\nabla f$ is $\beta$-Lipschitz, i.e
$$||\nabla f(x) - \nabla f(y)|| \leq \beta||x-y||$$

We will show that gradient descent  with update rule 
$$x_{t+1} = x_t - \eta \nabla f(x_t)$$
attains a faster rate of convergence under smoothness condition.  

\begin{theorem}
Let $f$ be convex and $\beta$-smooth on $\R^n$ then gradient descent with $\eta = \frac{1}{\beta}$ satsifies 
$$f(x_t) - f(x^*) \leq \frac{2\beta||x_1 - x^*||^2}{t-1}$$
\end{theorem}
To prove this we will need the following two lemmas.  

\begin{lemma}\label{l1}
Let $f$ be a $\beta$-smooth function on $\R^n$.  Then for any $x,y \in \R^n$, one has 
$$|f(x) - f(y) - \nabla f(y)^T(x-y)| \leq \frac{\beta}{2}||x-y||^2$$
\end{lemma}

\begin{proof}
First represent $f(x) - f(y)$ as an integral, apply Cauchy-Schwarz and then $\beta$-smoothness: 

$$|f(x) - f(y) - \nabla f(y)^T(x-y)|$$
$$= \big|\int\limits_{0}^1 \nabla f(y + t(x-y))^T(x-y)dt - \nabla f(y)^T(x-y)\big|$$
$$\leq  \int\limits_{0}^1 ||\nabla f(y + t(x-y)) - \nabla f(y)||\cdot ||x-y||dt$$
$$\leq \int\limits_{0}^1 \beta t||x-y||^2dt$$
$$= \frac{\beta}{2}||x-y||^2$$
\end{proof}

We also need 

\begin{lemma} \label{lm2}
Let $f$ be a $\beta$-smooth function, then for any $x,y \in \R^n$, one has 
$$f(x) - f(y)\leq \nabla f(x)^T(x-y) - \frac{1}{2\beta}||\nabla f(x) - \nabla f(y)||$$
\end{lemma}

\begin{proof}
Let $z = y - \frac{1}{\beta}(\nabla f(y) - \nabla f(x))$.  Then one has 
$$f(x) - f(y)$$
$$= f(x) - f(z) + f(z) - f(y)$$
$$\leq \nabla f(x)^T(x-z) + \nabla f(y)^T(z-y) + \frac{\beta}{2}||z-y||^2$$
$$= \nabla f(x)^T(x-y) + (\nabla f(x) - \nabla f(y))^T(y-z) + \frac{1}{2\beta}||\nabla f(x) - \nabla f(y)||^2$$
$$= \nabla f(x)^T(x-y) - \frac{1}{2\beta} ||\nabla f(x) - \nabla f(y)||^2$$
\end{proof}

Now we prove the theorem

\begin{proof}
By the update rule and lemma \ref{l1} we have 
$$f(x_{s+1}) - f(x_s) \leq -\frac{1}{2\beta}||\nabla f(x_s)||^2 $$ 
In particular, denoting $\delta_s = f(x_s) - f(x^*)$ this shows
$$\delta_{s+1} \leq \delta_s - \frac{1}{2\beta}||\nabla f(x_s)||^2 $$
One also has by convexity
$$\delta_s \leq \nabla f(x)s)^T(x_s - x^*) \leq ||x_s - x^*|| \cdot ||\nabla f(x_s)||$$
We will prove that $||x_s - x^*||$ is decreasing with $s$, which with the two above displays will imply 
$$\delta_{s+1}\leq \delta_s - \frac{1}{2\beta||x_1 - x^*||^2}\delta_s^2$$
We solve the recurrence as follows.  Let $w = \frac{1}{2\beta||x_1 - x^*||^2}$, then 
$$w\delta_s^2 + \delta_{s+1} \leq \delta_s \iff w\frac{\delta_s}{\delta_{s+1}} + \frac
{1}{\delta_s} \leq \frac{1}{\delta_{s+1}} \implies \frac{1}{\delta_{s+1}} - \frac{1}{\delta_s} \geq w \implies \frac{1}{\delta_t} \geq w(t-1)$$
To finish the proof it remains to show that $||x_2 - x^*||$ is decreasing with $s$.  Using lemma \ref{lm2} one immediately gets 
$$ (\nabla f(x) - \nabla f(y))^T(x - y) \geq \frac{1}{\beta} ||\nabla f(x) - \nabla f(y)||^2$$
We use this and the fact that $\nabla f(x^*) = 0$
$$||x_{s+1} - x^*||^2 = ||x_s - \frac{1}{\beta}\nabla f(x_s) - x^*||^2$$
$$= ||x_s - x^*||^2 - \frac{2}{\beta}\nabla f(x_s)^T(x_s - x^*) + \frac{1}{\beta^2}||nabla f(x_s)||^2 $$
$$\leq ||x_s - x^*||^2 - \frac{1}{\beta^2} ||\nabla f(x_s)||^2 $$
$$\leq ||x_s - x^*||^2$$
which concludes the proof.  
\end{proof}

